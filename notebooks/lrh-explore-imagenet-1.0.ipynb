{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from setup import setup\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lightning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 1281167\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "imagenet = datasets.load_dataset(\"ILSVRC/imagenet-1k\", trust_remote_code=True)\n",
    "print(imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.supcl.util import TwoCropTransform\n",
    "from torchvision import transforms\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        autoaugment.AutoAugment(policy=autoaugment.AutoAugmentPolicy.IMAGENET),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': [tensor([[[1.3584, 1.3584, 1.3584,  ..., 1.0331, 1.0331, 0.9988],\n",
       "           [1.4783, 1.4783, 1.4440,  ..., 1.1015, 1.1529, 1.1529],\n",
       "           [1.6153, 1.6153, 1.6324,  ..., 0.5364, 1.1529, 1.3070],\n",
       "           ...,\n",
       "           [1.4440, 1.4269, 1.4783,  ..., 1.5468, 1.5468, 1.5297],\n",
       "           [1.4098, 1.4269, 1.4269,  ..., 1.5297, 1.5468, 1.5468],\n",
       "           [1.4269, 1.4269, 1.4269,  ..., 1.5468, 1.5468, 1.5468]],\n",
       "  \n",
       "          [[1.5357, 1.5532, 1.5357,  ..., 1.2031, 1.1856, 1.1856],\n",
       "           [1.6758, 1.6758, 1.6583,  ..., 1.2556, 1.3431, 1.3431],\n",
       "           [1.8158, 1.8158, 1.7983,  ..., 0.5028, 1.3081, 1.5182],\n",
       "           ...,\n",
       "           [1.6057, 1.6408, 1.6583,  ..., 1.7108, 1.7108, 1.6933],\n",
       "           [1.6057, 1.6408, 1.6583,  ..., 1.6933, 1.7108, 1.7108],\n",
       "           [1.5882, 1.6057, 1.6408,  ..., 1.7108, 1.7108, 1.7108]],\n",
       "  \n",
       "          [[1.7860, 1.7860, 1.7860,  ..., 1.3851, 1.3677, 1.3677],\n",
       "           [1.9254, 1.9254, 1.8905,  ..., 1.4025, 1.5071, 1.5071],\n",
       "           [2.0474, 2.0474, 2.0300,  ..., 0.5485, 1.4374, 1.6640],\n",
       "           ...,\n",
       "           [1.8557, 1.8731, 1.8731,  ..., 1.8731, 1.8731, 1.8557],\n",
       "           [1.8557, 1.8731, 1.8731,  ..., 1.8905, 1.8731, 1.8731],\n",
       "           [1.8208, 1.8208, 1.8731,  ..., 1.8731, 1.9080, 1.9080]]]),\n",
       "  tensor([[[1.1015, 1.1015, 1.1187,  ..., 1.2899, 1.3070, 1.2899],\n",
       "           [1.0844, 1.0844, 1.1015,  ..., 1.3927, 1.5125, 1.4612],\n",
       "           [1.1015, 1.1187, 1.1529,  ..., 1.5810, 1.6324, 1.6324],\n",
       "           ...,\n",
       "           [1.5639, 1.3927, 1.0159,  ..., 1.9578, 1.9578, 1.9578],\n",
       "           [0.9646, 0.8789, 0.7419,  ..., 1.9920, 1.9749, 1.9235],\n",
       "           [1.6838, 1.5810, 1.7694,  ..., 1.9920, 1.9749, 1.9920]],\n",
       "  \n",
       "          [[1.2556, 1.3081, 1.3256,  ..., 1.4657, 1.4832, 1.5007],\n",
       "           [1.2556, 1.2731, 1.3081,  ..., 1.6408, 1.6933, 1.7283],\n",
       "           [1.3256, 1.3256, 1.3431,  ..., 1.8333, 1.8683, 1.8683],\n",
       "           ...,\n",
       "           [1.5707, 1.4132, 1.1155,  ..., 2.0084, 2.0084, 2.0609],\n",
       "           [0.9755, 0.9755, 0.9405,  ..., 2.0609, 2.0784, 2.0784],\n",
       "           [1.8508, 1.7458, 1.9384,  ..., 2.1485, 2.0959, 2.0959]],\n",
       "  \n",
       "          [[1.5245, 1.5420, 1.5594,  ..., 1.5942, 1.6291, 1.6291],\n",
       "           [1.4897, 1.5245, 1.5420,  ..., 1.7511, 1.7860, 1.8208],\n",
       "           [1.5420, 1.5594, 1.5942,  ..., 1.9603, 1.9777, 1.9951],\n",
       "           ...,\n",
       "           [1.7860, 1.5942, 1.2980,  ..., 2.1520, 2.1694, 2.2217],\n",
       "           [1.1585, 1.1585, 1.1585,  ..., 2.2217, 2.2391, 2.2217],\n",
       "           [2.0125, 1.9254, 2.1520,  ..., 2.3088, 2.2740, 2.2740]]])],\n",
       " 'label': 726}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.supcl.util import load_imagenet_hf\n",
    "imagenet_train = load_imagenet_hf(object(), TwoCropTransform(train_transform))['train']\n",
    "imagenet_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Threshold should be less than bound of img.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m auto_augment \u001b[38;5;241m=\u001b[39m autoaugment\u001b[38;5;241m.\u001b[39mAutoAugment(policy\u001b[38;5;241m=\u001b[39mautoaugment\u001b[38;5;241m.\u001b[39mAutoAugmentPolicy\u001b[38;5;241m.\u001b[39mIMAGENET)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Apply AutoAugment 3 times to get different random augmentations\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m aug1 \u001b[38;5;241m=\u001b[39m \u001b[43mauto_augment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m aug2 \u001b[38;5;241m=\u001b[39m auto_augment(image) \n\u001b[1;32m     15\u001b[0m aug3 \u001b[38;5;241m=\u001b[39m auto_augment(image)\n",
      "File \u001b[0;32m/opt/conda/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/lightning/lib/python3.10/site-packages/torchvision/transforms/autoaugment.py:279\u001b[0m, in \u001b[0;36mAutoAugment.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m signed \u001b[38;5;129;01mand\u001b[39;00m signs[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    278\u001b[0m             magnitude \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m--> 279\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43m_apply_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmagnitude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/envs/lightning/lib/python3.10/site-packages/torchvision/transforms/autoaugment.py:79\u001b[0m, in \u001b[0;36m_apply_op\u001b[0;34m(img, op_name, magnitude, interpolation, fill)\u001b[0m\n\u001b[1;32m     77\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mposterize(img, \u001b[38;5;28mint\u001b[39m(magnitude))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m op_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolarize\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmagnitude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m op_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoContrast\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     81\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mautocontrast(img)\n",
      "File \u001b[0;32m/opt/conda/envs/lightning/lib/python3.10/site-packages/torchvision/transforms/functional.py:1448\u001b[0m, in \u001b[0;36msolarize\u001b[0;34m(img, threshold)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39msolarize(img, threshold)\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/lightning/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:801\u001b[0m, in \u001b[0;36msolarize\u001b[0;34m(img, threshold)\u001b[0m\n\u001b[1;32m    798\u001b[0m _assert_channels(img, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m _max_value(img\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m--> 801\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThreshold should be less than bound of img.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    803\u001b[0m inverted_img \u001b[38;5;241m=\u001b[39m invert(img)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mwhere(img \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold, inverted_img, img)\n",
      "\u001b[0;31mTypeError\u001b[0m: Threshold should be less than bound of img."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms.autoaugment as autoaugment\n",
    "\n",
    "# Get first image from imagenet_train\n",
    "first_sample = imagenet_train[0]\n",
    "image = first_sample['image'][0] # Get first view of the two-crop transform\n",
    "\n",
    "# Create AutoAugment transform for ImageNet\n",
    "auto_augment = autoaugment.AutoAugment(policy=autoaugment.AutoAugmentPolicy.IMAGENET)\n",
    "\n",
    "# Apply AutoAugment 3 times to get different random augmentations\n",
    "aug1 = auto_augment(image)\n",
    "aug2 = auto_augment(image) \n",
    "aug3 = auto_augment(image)\n",
    "\n",
    "# Plot original and augmented images\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Helper function to denormalize and convert tensor to numpy for plotting\n",
    "def denormalize(tensor):\n",
    "    # Denormalize using ImageNet mean and std\n",
    "    denorm = transforms.Normalize(\n",
    "        mean=[-m/s for m, s in zip(mean, std)],\n",
    "        std=[1/s for s in std]\n",
    "    )\n",
    "    return denorm(tensor).permute(1, 2, 0).clip(0, 1).numpy()\n",
    "\n",
    "# Plot original and augmented images\n",
    "axes[0,0].imshow(denormalize(image))\n",
    "axes[0,0].set_title('Original')\n",
    "axes[0,1].imshow(denormalize(aug1))\n",
    "axes[0,1].set_title('AutoAugment 1')\n",
    "axes[1,0].imshow(denormalize(aug2))\n",
    "axes[1,0].set_title('AutoAugment 2') \n",
    "axes[1,1].imshow(denormalize(aug3))\n",
    "axes[1,1].set_title('AutoAugment 3')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 1281167\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets as torchvision_datasets\n",
    "from src.supcl.util import TwoCropTransform\n",
    "cifar10_dataset = torchvision_datasets.CIFAR10(\n",
    "    root='../datasets/', transform=TwoCropTransform(train_transform), download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[[-1.0219, -1.1589, -1.1418,  ..., -0.1143, -0.1143, -0.0972],\n",
       "           [-1.3987, -1.5699, -1.4672,  ..., -0.3712, -0.3883, -0.4568],\n",
       "           [-1.3815, -1.5014, -1.2959,  ..., -0.3198, -0.4226, -0.5253],\n",
       "           ...,\n",
       "           [ 0.3652,  0.0912, -0.0801,  ..., -0.2513,  0.2624,  0.2453],\n",
       "           [ 0.3823,  0.1426, -0.0116,  ..., -0.5253,  0.0741, -0.0287],\n",
       "           [ 0.2967,  0.1254,  0.0741,  ..., -1.0904, -0.3369, -0.1486]],\n",
       "  \n",
       "          [[-0.9153, -1.0553, -1.0378,  ...,  0.0126,  0.0126,  0.0301],\n",
       "           [-1.3004, -1.4755, -1.3704,  ..., -0.2500, -0.2675, -0.3375],\n",
       "           [-1.2829, -1.4055, -1.1954,  ..., -0.1975, -0.3025, -0.4076],\n",
       "           ...,\n",
       "           [ 0.5028,  0.2227,  0.0476,  ..., -0.1275,  0.3978,  0.3803],\n",
       "           [ 0.5203,  0.2752,  0.1176,  ..., -0.4076,  0.2052,  0.1001],\n",
       "           [ 0.4328,  0.2577,  0.2052,  ..., -0.9853, -0.2150, -0.0224]],\n",
       "  \n",
       "          [[-0.6890, -0.8284, -0.8110,  ...,  0.2348,  0.2348,  0.2522],\n",
       "           [-1.0724, -1.2467, -1.1421,  ..., -0.0267, -0.0441, -0.1138],\n",
       "           [-1.0550, -1.1770, -0.9678,  ...,  0.0256, -0.0790, -0.1835],\n",
       "           ...,\n",
       "           [ 0.7228,  0.4439,  0.2696,  ...,  0.0953,  0.6182,  0.6008],\n",
       "           [ 0.7402,  0.4962,  0.3393,  ..., -0.1835,  0.4265,  0.3219],\n",
       "           [ 0.6531,  0.4788,  0.4265,  ..., -0.7587,  0.0082,  0.1999]]]),\n",
       "  tensor([[[-0.4739, -0.4226, -0.4397,  ..., -1.0733, -1.2959, -1.2959],\n",
       "           [-0.4568, -0.4226, -0.4568,  ..., -0.8507, -0.9705, -1.0733],\n",
       "           [-0.4054, -0.4054, -0.4568,  ..., -0.6965, -0.6452, -0.8164],\n",
       "           ...,\n",
       "           [-0.0629, -0.2342, -0.3712,  ..., -0.0116, -0.8507, -1.1075],\n",
       "           [ 0.1083, -0.0116, -0.1657,  ..., -0.2342, -1.1247, -1.2103],\n",
       "           [ 0.2282,  0.1426,  0.0398,  ..., -0.2856, -1.1075, -1.0904]],\n",
       "  \n",
       "          [[-0.3550, -0.3025, -0.3200,  ..., -0.9678, -1.1954, -1.1954],\n",
       "           [-0.3375, -0.3025, -0.3375,  ..., -0.7402, -0.8627, -0.9678],\n",
       "           [-0.2850, -0.2850, -0.3375,  ..., -0.5826, -0.5301, -0.7052],\n",
       "           ...,\n",
       "           [ 0.0651, -0.1099, -0.2500,  ...,  0.1176, -0.7402, -1.0028],\n",
       "           [ 0.2402,  0.1176, -0.0399,  ..., -0.1099, -1.0203, -1.1078],\n",
       "           [ 0.3627,  0.2752,  0.1702,  ..., -0.1625, -1.0028, -0.9853]],\n",
       "  \n",
       "          [[-0.1312, -0.0790, -0.0964,  ..., -0.7413, -0.9678, -0.9678],\n",
       "           [-0.1138, -0.0790, -0.1138,  ..., -0.5147, -0.6367, -0.7413],\n",
       "           [-0.0615, -0.0615, -0.1138,  ..., -0.3578, -0.3055, -0.4798],\n",
       "           ...,\n",
       "           [ 0.2871,  0.1128, -0.0267,  ...,  0.3393, -0.5147, -0.7761],\n",
       "           [ 0.4614,  0.3393,  0.1825,  ...,  0.1128, -0.7936, -0.8807],\n",
       "           [ 0.5834,  0.4962,  0.3916,  ...,  0.0605, -0.7761, -0.7587]]])],\n",
       " 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
